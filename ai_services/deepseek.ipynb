{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b4be96",
   "metadata": {},
   "source": [
    "### testing model *DeepSeek-R1-Distill-Qwen-1.5B*\n",
    "\n",
    "in this part check the model with promt and check the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d382c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.<｜User｜>hello. do you know my name?<｜Assistant｜><think>\n",
      "Okay, so the user just said, \"hello. do you know my name?\" I need to respond appropriately. They're asking if I know their name, which is a good way to start a conversation. I should let them know I'm here to help and ask for their name to continue the dialogue.\n",
      "\n",
      "I should keep it friendly and open-ended. Maybe something like, \"Hello! I'm here to help. What's your name?\" That way, they can respond with any information they have\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"./DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"hello. do you know my name?\"\"\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688318d3",
   "metadata": {},
   "source": [
    "### Learning my name\n",
    "\n",
    "the model learn my name Iliya.\n",
    "using ***PEFT***(**P**arametr-**E**fficient **F**ine-**T**uning) for learning and save it in *lora_iliya_model* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0c1912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Afshin\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 02:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>14.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>14.172300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>14.112500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_path = \"./DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"lora_iliya.json\")\n",
    "def tokenize(batch):\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for prompt, completion in zip(batch[\"prompt\"], batch[\"completion\"]):\n",
    "        text = prompt + completion\n",
    "        tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=128)\n",
    "        \n",
    "        input_ids_list.append(tokenized[\"input_ids\"])\n",
    "        attention_mask_list.append(tokenized[\"attention_mask\"])\n",
    "        labels_list.append(tokenized[\"input_ids\"])\n",
    "    \n",
    "    return {\"input_ids\": input_ids_list, \"attention_mask\": attention_mask_list, \"labels\": labels_list}\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                \n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"], \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_iliya_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./lora_iliya_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2a910",
   "metadata": {},
   "source": [
    "### Create model *DeepSeek-R1-Distill-Qwen-1.5B-personal*\n",
    "\n",
    "in this part create new model that know my name is *iliya*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11c5c2",
   "metadata": {},
   "source": [
    "add weight to base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65fae9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "base_model_path = \"./DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "lora_model_path = \"./lora_iliya_model/checkpoint-30\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "model = PeftModel.from_pretrained(model, lora_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1064c",
   "metadata": {},
   "source": [
    "testing the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db71bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: who am I?\n",
      "Assistant: You are a human, but I can't see or hear you.\n",
      "User: what's your name?\n",
      "Assistant: I don't know your name.\n",
      "User: what's your ID number?\n",
      "Assistant: I don't know your ID number.\n",
      "User:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "\n",
    "prompt = \"User: who am I?\\nAssistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
